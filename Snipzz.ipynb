import os
import faiss
import mammoth
import numpy as np
from docx import Document as DocxDocument
from langchain.docstore.document import Document
from openpyxl import load_workbook
import PyPDF2
import fitz
import xlrd
import subprocess
from langchain.embeddings.vertexai import VertexAIEmbeddings
from langchain.llms import VertexAI
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import FAISS
import faiss
import pickle


def read_txt(file_path):
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
        return file.read()

def read_pdf(file_path):
    text = ""
    try:
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            for page_num in range(len(pdf_reader.pages)):
                text += pdf_reader.pages[page_num].extract_text()
    except:
        pdf_document = fitz.open(file_path)
        for page_num in range(pdf_document.page_count):
            page = pdf_document.load_page(page_num)
            text += page.get_text()
    return text

def read_docx(file_path):
    doc = DocxDocument(file_path)
    return "\n".join([paragraph.text for paragraph in doc.paragraphs])

def read_docm(file_path):
    with open(file_path, 'rb') as docm_file:
        result = mammoth.extract_raw_text(docm_file)
        return result.value

def read_xls(file_path):
    workbook = xlrd.open_workbook(file_path)
    sheet = workbook.sheet_by_index(0)
    data = []
    for row_num in range(sheet.nrows):
        data.append("\t".join(map(str, sheet.row_values(row_num))))
    return "\n".join(data)

def read_xlsx(file_path):
    workbook = load_workbook(file_path)
    sheet = workbook.active
    data = []
    for row in sheet.iter_rows(values_only=True):
        data.append("\t".join(map(str, row)))
    return "\n".join(data)

def read_doc(file_path):
    result = subprocess.run(['antiword',file_path],capture_output=True, text=True)
    if result.returncode != 0:
        raise Exception(f"Error reading {file_path}: {result.stderr}")
    return result.stdout

def create_faiss_index(embedding_dim):
    index=faiss.IndexFlatL2(embedding_dim)
    return index


def load_files(directory,embeddings_model):
    documents = []
    unread_documents = []
    embed_doc=[]
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2500, chunk_overlap=250)
    total = 0
    for filename in os.listdir(directory):
        total += 1
        file_path = os.path.join(directory, filename)
        try:
            if filename.endswith('.txt'):
                content = read_txt(file_path)
            elif filename.endswith('.pdf'):
                content = read_pdf(file_path)
            elif filename.endswith('.docx'):
                content = read_docx(file_path)
            elif filename.endswith('.doc'):
                content = read_doc(file_path)
            elif filename.endswith('.docm'):
                content = read_docm(file_path)
            elif filename.endswith('.xls'):
                content = read_xls(file_path)
            elif filename.endswith('.xlsx'):
                content = read_xlsx(file_path)
            else:
                print(f"Unsupported file format: {filename}")
                unread_documents.append(filename)
                continue
            doc = Document(page_content=content, metadata={"filename": filename})
            documents.append(doc)
            
            splits = text_splitter.split_documents([doc])
            embeddings= embed_documents(splits,embeddings_model)
            embed_doc.extend(embeddings)
        except Exception as e:
            print(f"Failed to process {filename}: {e}")
            unread_documents.append(filename)
    return documents, unread_documents, total,embed_doc



# def batch_documents(documents, batch_size=100):
#     return [documents[i:i + batch_size] for i in range(0, len(documents), batch_size)]

def embed_documents(documents,embedding_model):
    embeddings = []
    for doc in documents:
        embedding = embedding_model.embed_documents([doc.page_content])
        embeddings.extend(embedding)
    return embeddings

def initialize_models():
    # Initialize VertexAI models
    llm = VertexAI(model="gemini-pro", top_k=5, top_p=0.9, temperature=0.7, max_output_tokens=2048)
    embeddings = VertexAIEmbeddings(model_name="textembedding-gecko@003")
    return llm, embeddings

dinlei

def create_and_save_faiss_index(documents, embeddings_model, batch_size=1000, index_path='faiss_index.bin', docs_path='documents.pkl'):
    faiss_indices = []
    all_documents = []
    
    for i in range(0, len(documents), batch_size):
        batch_docs = documents[i:i + batch_size]
        faiss_index = FAISS.from_documents(batch_docs, embeddings_model)
        faiss_indices.append(faiss_index)
        all_documents.extend(batch_docs)
    
    # Merge all FAISS indices into one
    final_index = faiss_indices[0]
    for index in faiss_indices[1:]:
        final_index.merge_from(index)
    
    # Save final FAISS index
    faiss.write_index(final_index.index, index_path)
    
    # Save Document objects
    with open(docs_path, 'wb') as f:
        pickle.dump(all_documents, f)

# Initialize models
llm, embeddings_model = initialize_models()
batch_size=100
embedding_dim=768

# Load documents
directory = '/home'
documents, unread, total,embedd = load_files(directory,embeddings_model)

def create_and_save_faiss_index(documents, embeddings_model, batch_size=1000, index_path='faiss_index.bin', docs_path='documents.pkl'):
    all_embeddings = []
    all_documents = []

    for i in range(0, len(documents), batch_size):
        batch_docs = documents[i:i + batch_size]
        faiss_index = FAISS.from_documents(batch_docs, embeddings_model)
        all_embeddings.append(faiss_index.index.reconstruct_n(0, faiss_index.index.ntotal))
        all_documents.extend(batch_docs)
    
    # Concatenate all embeddings
    embeddings = np.vstack(all_embeddings)
    
    # Create final FAISS index
    dimension = embeddings.shape[1]
    final_index = faiss.IndexFlatL2(dimension)
    final_index.add(embeddings)

    # Save final FAISS index
    faiss.write_index(final_index, index_path)
    
    # Save Document objects
    with open(docs_path, 'wb') as f:
        pickle.dump(all_documents, f)

