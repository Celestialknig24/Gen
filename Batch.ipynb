import os
import fitz  # PyMuPDF for PDF files
from docx import Document as DocxDocument  # python-docx for DOCX files
import pandas as pd  # pandas for XLSX files

class Document:
    def __init__(self, text):
        self.text = text

def load_pdf(file_path):
    try:
        doc = fitz.open(file_path)
        text = ""
        for page in doc:
            text += page.get_text()
        return Document(text)
    except Exception as e:
        print(f"Error loading PDF file {file_path}: {e}")
        return None

def load_docx(file_path):
    try:
        doc = DocxDocument(file_path)
        text = "\n".join([para.text for para in doc.paragraphs])
        return Document(text)
    except Exception as e:
        print(f"Error loading DOCX file {file_path}: {e}")
        return None

def load_txt(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            text = file.read()
        return Document(text)
    except Exception as e:
        print(f"Error loading TXT file {file_path}: {e}")
        return None

def load_xlsx_with_pandas(file_path):
    try:
        df = pd.read_excel(file_path)
        text = df.to_string(index=False)
        return Document(text)
    except Exception as e:
        print(f"Error loading XLSX file {file_path} with pandas: {e}")
        return None

def load_document(file_path):
    ext = os.path.splitext(file_path)[1].lower()
    if ext == '.pdf':
        return load_pdf(file_path)
    elif ext == '.docx':
        return load_docx(file_path)
    elif ext == '.txt':
        return load_txt(file_path)
    elif ext == '.xlsx':
        return load_xlsx_with_pandas(file_path)
    else:
        print(f"Unsupported file extension: {ext}")
        return None

def file_loader(folder_path):
    data = []
    unread_files = []

    for root, _, files in os.walk(folder_path):
        for file in files:
            file_path = os.path.join(root, file)
            doc = load_document(file_path)
            if doc:
                data.append(doc)
                print(f"Successfully processed file {file_path}")
            else:
                unread_files.append(file_path)

    print(f"Processed {len(data)} files. {len(unread_files)} files could not be read.")
    return data, unread_files

class RecursiveCharacterTextSplitter:
    def __init__(self, chunk_size, chunk_overlap):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def split_documents(self, documents):
        splits = []
        for document in documents:
            text = document.text
            start = 0
            while start < len(text):
                end = min(start + self.chunk_size, len(text))
                splits.append(text[start:end])
                start = end - self.chunk_overlap if end < len(text) else end
        return splits

def split_documents(documents):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=19000, chunk_overlap=2090)
    splits = text_splitter.split_documents(documents)
    return splits

def batch_splits(splits, max_tokens=20000):
    batches = []
    current_batch = []
    current_token_count = 0

    for split in splits:
        split_token_count = len(split.split())
        if current_token_count + split_token_count <= max_tokens:
            current_batch.append(split)
            current_token_count += split_token_count
        else:
            batches.append(current_batch)
            current_batch = [split]
            current_token_count = split_token_count

    if current_batch:
        batches.append(current_batch)

    return batches

def vectorize_documents(batch):
    # Example placeholder for vectorizing documents
    # Replace this with your actual vectorization logic
    for document in batch:
        pass  # Implement your vectorization logic here

def process_batches(batches):
    for i, batch in enumerate(batches):
        print(f"Processing batch {i+1}/{len(batches)} with {sum(len(split.split()) for split in batch)} tokens")
        # Process your batch here, e.g., vectorizing the documents
        vectorize_documents(batch)

# Usage
folder_path = 'path_to_your_folder'  # Replace with your actual folder path
documents, unread_files = file_loader(folder_path)
splits = split_documents(documents)

# Batch the splits into groups of up to 20,000 tokens
batches = batch_splits(splits, max_tokens=20000)

# Process each batch
process_batches(batches)

# Output splits for verification
print(f"Total number of batches: {len(batches)}")
