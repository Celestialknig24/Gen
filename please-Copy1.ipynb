{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1249851-cb14-49f7-9bc4-1d21bf2deec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mammoth\n",
    "import numpy as np\n",
    "# from docx import Document as DocxDocument\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.docstore.base import Docstore\n",
    "from openpyxl import load_workbook\n",
    "# import PyPDF2\n",
    "# import fitz\n",
    "# import xlrd\n",
    "import subprocess\n",
    "from langchain.embeddings.vertexai import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import faiss\n",
    "import pickle\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b84af24-a589-4033-8deb-11c8150a0e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models():\n",
    "    # Initialize VertexAI models\n",
    "    llm = VertexAI(model=\"gemini-pro\", top_k=5, top_p=0.9, temperature=0.7, max_output_tokens=2048)\n",
    "    embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@latest\")\n",
    "    return llm, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e474e6a-fd4d-4386-b253-170764ca1222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `VertexAI` was deprecated in LangChain 0.0.12 and will be removed in 0.2.0. An updated version of the class exists in the langchain-google-vertexai package and should be used instead. To use it run `pip install -U langchain-google-vertexai` and import as `from langchain_google_vertexai import VertexAI`.\n",
      "  warn_deprecated(\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `VertexAIEmbeddings` was deprecated in LangChain 0.0.12 and will be removed in 0.2.0. An updated version of the class exists in the langchain-google-vertexai package and should be used instead. To use it run `pip install -U langchain-google-vertexai` and import as `from langchain_google_vertexai import VertexAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm, embeddings = initialize_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27688a42-1786-44f9-aed7-f875b749f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_faiss_index(documents, embeddings_model, combined_path='all_embeddings_and_texts.pkl', batch_size=30):\n",
    "    all_texts = []\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch_docs = documents[i:i + batch_size]\n",
    "        texts = [doc.page_content for doc in batch_docs]\n",
    "        embeddings = embeddings_model.embed_documents(texts)\n",
    "        \n",
    "        all_texts.extend(texts)\n",
    "        all_embeddings.extend(embeddings)\n",
    "    \n",
    "    # faiss_index = FAISS.from_embeddings(all_embeddings, all_texts)\n",
    "    \n",
    "    with open(combined_path, 'wb') as f:\n",
    "        pickle.dump({'all_embeddings': all_embeddings, 'all_texts': all_texts}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "387fcd38-0a09-4115-b028-4a90c27dd757",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path='documents_obj.pkl'\n",
    "with open(doc_path,'rb') as f:\n",
    "    documents=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5380408-3733-42ab-93e5-fad4ec2879cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68662"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aa3d131-1d7b-4bdf-9a00-ef319ee43f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_save_faiss_index(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ea5e71-6757-498e-9c58-dc4cbd150f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path='all_embeddings_and_texts.pkl'\n",
    "with open(doc_path,'rb') as f:\n",
    "    all=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "324b3bcc-6fe2-4426-9a2a-d20bcd5bfc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb=all['all_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "257848a3-cd85-4a68-8f79-53df7cf8aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=all['all_texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "985f7b44-326c-468a-9aaf-7812336b7a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "rey=list(zip(txt,emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b0f32cf-d3b1-49b3-a226-1b459fe88632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68662"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abc323bc-787a-4ee7-92a8-290ce2b57fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index = FAISS.from_embeddings(rey,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ddee519-ec94-45bc-8b4f-f6ebf14d14fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index.save_local('sharepoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "461f83c0-278d-41b0-b731-27055d750785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(vectorstore):\n",
    "    # Create retriever\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    print('create_retriever')\n",
    "    return retriever\n",
    "\n",
    "def define_prompts():\n",
    "    # Define prompts\n",
    "    contextualize_q_system_prompt = \"Given a chat history and the latest user question which might reference content in the chat history, formulate a standalone question which can be understood without the chat history. Do Not answer the question, just refromulate it if needed and otherwise return it as is.\"\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "    print('define_prompts')\n",
    "    contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
    "    qa_system_prompt = \"You are an assistant for question answering tasks. Use only the following pieces of retrieved context to answer the question. If the question is not related to the context then don't answer, just say that you are not sure about that. If you don't know the answer, just say that you are not sure about that in 1 or 2 lines and strictly dont exceed more than that. Question: {question} Context: {context} Answer:\"\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", qa_system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"human\", \"{question}\"),\n",
    "        ]\n",
    "    )\n",
    "    return contextualize_q_chain, qa_prompt\n",
    "\n",
    "def define_chain(contextualize_q_chain, qa_prompt, retriever):\n",
    "    # Define chain\n",
    "    rag_chain = (\n",
    "        RunnablePassthrough.assign(context=contextualized_question | retriever | format_docs) | qa_prompt | llm\n",
    "    )\n",
    "    print('define_chain')\n",
    "    return rag_chain\n",
    "\n",
    "def contextualized_question(input):\n",
    "    \n",
    "    if input.get(\"chat_history\"):\n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input['question']\n",
    "    \n",
    "def format_docs(docs):\n",
    "    formatted_docs = \"\\n\\n\".join(f\"{doc.page_content} (Source: {doc.metadata['source']})\" if 'source' in doc.metadata else f\"{doc.page_content}\" for doc in docs)\n",
    "    return formatted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44d7d817-a2fe-4ebe-8bdd-9dc201f70154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, FewShotPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d505bed-dcdc-4764-b3af-f4b1694cb5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_retriever\n",
      "define_prompts\n",
      "define_chain\n"
     ]
    }
   ],
   "source": [
    "# Create retriever\n",
    "retriever = create_retriever(faiss_index)\n",
    "\n",
    "# Define prompts\n",
    "contextualize_q_chain, qa_prompt = define_prompts()\n",
    "\n",
    "# Define chain\n",
    "rag_chain = define_chain(contextualize_q_chain, qa_prompt, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8efe71f-5c0e-432a-a539-947131bc9af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assistant: Yes, Tech2go is a mobile application available for Android devices. It can be installed from the Play Store.\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke({\"chat_history\":[],\"question\":\"Tech2go is mobile application?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dbea8cd-6a6d-438e-8c45-0b6eebfff342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The provided text focuses on instructions and guidelines for using the Tech2go mobile application. It covers various aspects such as installation, settings, job management, and troubleshooting, but it doesn't specifically mention the features of Tech2go. Therefore, I cannot answer the question based on the given context.\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke({\"chat_history\":[],\"question\":\"features of tech2go?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb0743-6ede-4021-954f-a3f7287ff887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b980552-ef2e-4bd3-8920-51551cdfcb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "local-conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
