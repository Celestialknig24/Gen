
def create_and_save_faiss_index_alt(documents, embeddings_model, batch_size=60, index_path='temp.bin', docs_path='tempo.pkl'):
    all_embeddings = []
    index_to_docstore_id = {}
    
    # Process documents in batches
    for i in range(0, len(documents), batch_size):
        batch_docs = documents[i:i + batch_size]
        batch_embeddings = embeddings_model(batch_docs)  # Assume embeddings_model can handle batching
        all_embeddings.append(batch_embeddings)
        
        for j, doc in enumerate(batch_docs):
            index_to_docstore_id[len(index_to_docstore_id)] = doc
    
    # Concatenate all embeddings
    embeddings = np.vstack(all_embeddings)
    
    # Create final FAISS index
    dimension = embeddings.shape[1]
    final_index = faiss.IndexFlatL2(dimension)
    final_index.add(embeddings)
    
    # Save the FAISS index
    faiss.write_index(final_index, index_path)
    
    # Save Document objects
    with open(docs_path, 'wb') as f:
        pickle.dump(documents, f)
    
    # Create Docstore
    docstore = Docstore(dict(enumerate(documents)))
    
    # Create FAISS vectorstore object
    faiss_vectorstore = FAISS(index=final_index, docstore=docstore, embedding_function=embeddings_model, index_to_docstore_id=index_to_docstore_id)
    return faiss_vectorstore









import os
import mammoth
import numpy as np
from docx import Document as DocxDocument
from langchain.docstore.document import Document
from openpyxl import load_workbook
import PyPDF2
import fitz
import xlrd
import subprocess
from langchain.embeddings.vertexai import VertexAIEmbeddings
from langchain.llms import VertexAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
import pickle


def read_txt(file_path):
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
        return file.read()


def read_pdf(file_path):
    text = ""
    try:
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            for page_num in range(len(pdf_reader.pages)):
                text += pdf_reader.pages[page_num].extract_text()
    except:
        pdf_document = fitz.open(file_path)
        for page_num in range(pdf_document.page_count):
            page = pdf_document.load_page(page_num)
            text += page.get_text()
    return text


def read_docx(file_path):
    doc = DocxDocument(file_path)
    return "\n".join([paragraph.text for paragraph in doc.paragraphs])


def read_docm(file_path):
    with open(file_path, 'rb') as docm_file:
        result = mammoth.extract_raw_text(docm_file)
        return result.value


def read_xls(file_path):
    workbook = xlrd.open_workbook(file_path)
    sheet = workbook.sheet_by_index(0)
    data = []
    for row_num in range(sheet.nrows):
        data.append("\t".join(map(str, sheet.row_values(row_num))))
    return "\n".join(data)


def read_xlsx(file_path):
    workbook = load_workbook(file_path)
    sheet = workbook.active
    data = []
    for row in sheet.iter_rows(values_only=True):
        data.append("\t".join(map(str, row)))
    return "\n".join(data)


def read_doc(file_path):
    result = subprocess.run(['antiword', file_path], capture_output=True, text=True)
    if result.returncode != 0:
        raise Exception(f"Error reading {file_path}: {result.stderr}")
    return result.stdout


def load_files(directory):
    documents = []
    unread_documents = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)
        try:
            if filename.endswith('.txt'):
                content = read_txt(file_path)
            elif filename.endswith('.pdf'):
                content = read_pdf(file_path)
            elif filename.endswith('.docx'):
                content = read_docx(file_path)
            elif filename.endswith('.doc'):
                content = read_doc(file_path)
            elif filename.endswith('.docm'):
                content = read_docm(file_path)
            elif filename.endswith('.xls'):
                content = read_xls(file_path)
            elif filename.endswith('.xlsx'):
                content = read_xlsx(file_path)
            else:
                print(f"Unsupported file format: {filename}")
                unread_documents.append(filename)
                continue
            doc = Document(page_content=content, metadata={"filename": filename})
            split_docs = text_splitter.split_documents([doc])
            documents.extend(split_docs)
        except Exception as e:
            print(f"Failed to process {filename}: {e}")
            unread_documents.append(filename)
    return documents, unread_documents


def initialize_models():
    # Initialize VertexAI models
    llm = VertexAI(model="gemini-pro", top_k=5, top_p=0.9, temperature=0.7, max_output_tokens=2048)
    embeddings = VertexAIEmbeddings(model_name="textembedding-gecko@003")
    return llm, embeddings


def create_and_save_faiss_index(documents, embeddings_model, combined_path='faiss_and_docs.pkl'):
    faiss_index = FAISS.from_documents(documents, embeddings_model)
    with open(combined_path, 'wb') as f:
        pickle.dump({'index': faiss_index, 'documents': documents}, f)


def load_faiss_index(combined_path='faiss_and_docs.pkl'):
    with open(combined_path, 'rb') as f:
        data = pickle.load(f)
    return data['index'], data['documents']


def perform_rag(query, faiss_index, embedding_model):
    query_embedding = embedding_model.embed_documents([query])[0]
    query_embedding_array = np.array([query_embedding]).astype(np.float32)
    D, I = faiss_index.index.search(query_embedding_array, k=5)  # Retrieve top 5 results
    results = [faiss_index.index_to_docstore_id(i) for i in I[0]]
    return results


# Main execution
if __name__ == "__main__":
    # Initialize models
    llm, embeddings_model = initialize_models()

    # Load documents
    directory = '/path_to_your_directory'
    documents, unread_documents = load_files(directory)

    # Create and save FAISS index
    create_and_save_faiss_index(documents, embeddings_model, combined_path='faiss_and_docs.pkl')

    # Load FAISS index and perform RAG
    faiss_index, documents = load_faiss_index()
    query = "example query text"
    results = perform_rag(query, faiss_index, embeddings_model)

    print("Retrieved Results:")
    for result in results:
        print(result.metadata["filename"], result.page_content)
