
def create_and_save_faiss_index_alt(documents, embeddings_model, batch_size=60, index_path='temp.bin', docs_path='tempo.pkl'):
    all_embeddings = []
    index_to_docstore_id = {}
    
    # Process documents in batches
    for i in range(0, len(documents), batch_size):
        batch_docs = documents[i:i + batch_size]
        batch_embeddings = embeddings_model(batch_docs)  # Assume embeddings_model can handle batching
        all_embeddings.append(batch_embeddings)
        
        for j, doc in enumerate(batch_docs):
            index_to_docstore_id[len(index_to_docstore_id)] = doc
    
    # Concatenate all embeddings
    embeddings = np.vstack(all_embeddings)
    
    # Create final FAISS index
    dimension = embeddings.shape[1]
    final_index = faiss.IndexFlatL2(dimension)
    final_index.add(embeddings)
    
    # Save the FAISS index
    faiss.write_index(final_index, index_path)
    
    # Save Document objects
    with open(docs_path, 'wb') as f:
        pickle.dump(documents, f)
    
    # Create Docstore
    docstore = Docstore(dict(enumerate(documents)))
    
    # Create FAISS vectorstore object
    faiss_vectorstore = FAISS(index=final_index, docstore=docstore, embedding_function=embeddings_model, index_to_docstore_id=index_to_docstore_id)
    return faiss_vectorstore


---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[17], line 1
----> 1 vs= create_and_save_faiss_index(documents, embeddings)

Cell In[14], line 8, in create_and_save_faiss_index(documents, embeddings_model, batch_size, index_path, docs_path)
      6 for i in range(0, len(documents), batch_size):
      7     batch_docs = documents[i:i + batch_size]
----> 8     batch_embeddings = embeddings_model.embed(batch_docs)
      9     final_index.add(batch_embeddings)
     10     docstore.update({i + j: doc for j, doc in enumerate(batch_docs)})

File /opt/conda/lib/python3.10/site-packages/langchain_community/embeddings/vertexai.py:300, in VertexAIEmbeddings.embed(self, texts, batch_size, embeddings_task_type)
    297     batches = VertexAIEmbeddings._prepare_batches(texts, batch_size)
    298 else:
    299     # Dynamic batch size, starting from 250 at the first call.
--> 300     first_batch_result, batches = self._prepare_and_validate_batches(
    301         texts, embeddings_task_type
    302     )
    303 # First batch result may have some embeddings already.
    304 # In such case, batches have texts that were not processed yet.
    305 embeddings.extend(first_batch_result)

File /opt/conda/lib/python3.10/site-packages/langchain_community/embeddings/vertexai.py:195, in VertexAIEmbeddings._prepare_and_validate_batches(self, texts, embeddings_type)
    188 """Prepares text batches with one-time validation of batch size.
    189 Batch size varies between GCP regions and individual project quotas.
    190 # Returns embeddings of the first text batch that went through,
    191 # and text batches for the rest of the texts.
    192 """
    193 from google.api_core.exceptions import InvalidArgument
--> 195 batches = VertexAIEmbeddings._prepare_batches(
    196     texts, self.instance["batch_size"]
    197 )
    198 # If batch size if less or equal to one that went through before,
    199 # then keep batches as they are.
    200 if len(batches[0]) <= self.instance["min_good_batch_size"]:

File /opt/conda/lib/python3.10/site-packages/langchain_community/embeddings/vertexai.py:116, in VertexAIEmbeddings._prepare_batches(texts, batch_size)
    109 current_text = texts[text_index]
    110 # Number of tokens per a text is conservatively estimated
    111 # as 2 times number of words, punctuation and whitespace characters.
    112 # Using `count_tokens` API will make batching too expensive.
    113 # Utilizing a tokenizer, would add a dependency that would not
    114 # necessarily be reused by the application using this class.
    115 current_text_token_cnt = (
--> 116     len(VertexAIEmbeddings._split_by_punctuation(current_text)) * 2
    117 )
    118 end_of_batch = False
    119 if current_text_token_cnt > _MAX_TOKENS_PER_BATCH:
    120     # Current text is too big even for a single batch.
    121     # Such request will fail, but we still make a batch
    122     # so that the app can get the error from the API.

File /opt/conda/lib/python3.10/site-packages/langchain_community/embeddings/vertexai.py:94, in VertexAIEmbeddings._split_by_punctuation(text)
     92 pattern = f"([{split_by}])"
     93 # Using re.split to split the text based on the pattern
---> 94 return [segment for segment in re.split(pattern, text) if segment]

File /opt/conda/lib/python3.10/re.py:230, in split(pattern, string, maxsplit, flags)
    222 def split(pattern, string, maxsplit=0, flags=0):
    223     """Split the source string by the occurrences of the pattern,
    224     returning a list containing the resulting substrings.  If
    225     capturing parentheses are used in pattern, then the text of all
   (...)
    228     and the remainder of the string is returned as the final element
    229     of the list."""
--> 230     return _compile(pattern, flags).split(string, maxsplit)

TypeError: expected string or bytes-like object






import os
import mammoth
import numpy as np
from docx import Document as DocxDocument
from langchain.docstore.document import Document
from openpyxl import load_workbook
import PyPDF2
import fitz
import xlrd
import subprocess
from langchain.embeddings.vertexai import VertexAIEmbeddings
from langchain.llms import VertexAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
import pickle


def read_txt(file_path):
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
        return file.read()


def read_pdf(file_path):
    text = ""
    try:
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            for page_num in range(len(pdf_reader.pages)):
                text += pdf_reader.pages[page_num].extract_text()
    except:
        pdf_document = fitz.open(file_path)
        for page_num in range(pdf_document.page_count):
            page = pdf_document.load_page(page_num)
            text += page.get_text()
    return text


def read_docx(file_path):
    doc = DocxDocument(file_path)
    return "\n".join([paragraph.text for paragraph in doc.paragraphs])


def read_docm(file_path):
    with open(file_path, 'rb') as docm_file:
        result = mammoth.extract_raw_text(docm_file)
        return result.value


def read_xls(file_path):
    workbook = xlrd.open_workbook(file_path)
    sheet = workbook.sheet_by_index(0)
    data = []
    for row_num in range(sheet.nrows):
        data.append("\t".join(map(str, sheet.row_values(row_num))))
    return "\n".join(data)


def read_xlsx(file_path):
    workbook = load_workbook(file_path)
    sheet = workbook.active
    data = []
    for row in sheet.iter_rows(values_only=True):
        data.append("\t".join(map(str, row)))
    return "\n".join(data)


def read_doc(file_path):
    result = subprocess.run(['antiword', file_path], capture_output=True, text=True)
    if result.returncode != 0:
        raise Exception(f"Error reading {file_path}: {result.stderr}")
    return result.stdout


def load_files(directory):
    documents = []
    unread_documents = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)
        try:
            if filename.endswith('.txt'):
                content = read_txt(file_path)
            elif filename.endswith('.pdf'):
                content = read_pdf(file_path)
            elif filename.endswith('.docx'):
                content = read_docx(file_path)
            elif filename.endswith('.doc'):
                content = read_doc(file_path)
            elif filename.endswith('.docm'):
                content = read_docm(file_path)
            elif filename.endswith('.xls'):
                content = read_xls(file_path)
            elif filename.endswith('.xlsx'):
                content = read_xlsx(file_path)
            else:
                print(f"Unsupported file format: {filename}")
                unread_documents.append(filename)
                continue
            doc = Document(page_content=content, metadata={"filename": filename})
            split_docs = text_splitter.split_documents([doc])
            documents.extend(split_docs)
        except Exception as e:
            print(f"Failed to process {filename}: {e}")
            unread_documents.append(filename)
    return documents, unread_documents


def initialize_models():
    # Initialize VertexAI models
    llm = VertexAI(model="gemini-pro", top_k=5, top_p=0.9, temperature=0.7, max_output_tokens=2048)
    embeddings = VertexAIEmbeddings(model_name="textembedding-gecko@003")
    return llm, embeddings


def create_and_save_faiss_index(documents, embeddings_model, combined_path='faiss_and_docs.pkl'):
    faiss_index = FAISS.from_documents(documents, embeddings_model)
    with open(combined_path, 'wb') as f:
        pickle.dump({'index': faiss_index, 'documents': documents}, f)


def load_faiss_index(combined_path='faiss_and_docs.pkl'):
    with open(combined_path, 'rb') as f:
        data = pickle.load(f)
    return data['index'], data['documents']


def perform_rag(query, faiss_index, embedding_model):
    query_embedding = embedding_model.embed_documents([query])[0]
    query_embedding_array = np.array([query_embedding]).astype(np.float32)
    D, I = faiss_index.index.search(query_embedding_array, k=5)  # Retrieve top 5 results
    results = [faiss_index.index_to_docstore_id(i) for i in I[0]]
    return results


# Main execution
if __name__ == "__main__":
    # Initialize models
    llm, embeddings_model = initialize_models()

    # Load documents
    directory = '/path_to_your_directory'
    documents, unread_documents = load_files(directory)

    # Create and save FAISS index
    create_and_save_faiss_index(documents, embeddings_model, combined_path='faiss_and_docs.pkl')

    # Load FAISS index and perform RAG
    faiss_index, documents = load_faiss_index()
    query = "example query text"
    results = perform_rag(query, faiss_index, embeddings_model)

    print("Retrieved Results:")
    for result in results:
        print(result.metadata["filename"], result.page_content)
