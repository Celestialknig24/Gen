import os
import mammoth
import numpy as np
from docx import Document as DocxDocument
from langchain.docstore.document import Document
from langchain.docstore.base import Docstore
from openpyxl import load_workbook
import PyPDF2
import fitz
import xlrd
import subprocess
from langchain.embeddings.vertexai import VertexAIEmbeddings
from langchain.llms import VertexAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
import faiss
import pickle
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

def read_txt(file_path):
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
        return file.read()

def read_pdf(file_path):
    text = ""
    try:
        with open(file_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            for page_num in range(len(pdf_reader.pages)):
                text += pdf_reader.pages[page_num].extract_text()
    except:
        pdf_document = fitz.open(file_path)
        for page_num in range(pdf_document.page_count):
            page = pdf_document.load_page(page_num)
            text += page.get_text()
    return text

def read_docx(file_path):
    doc = DocxDocument(file_path)
    return "\n".join([paragraph.text for paragraph in doc.paragraphs])

def read_xls(file_path):
    workbook = xlrd.open_workbook(file_path)
    sheet = workbook.sheet_by_index(0)
    data = []
    for row_num in range(sheet.nrows):
        data.append("\t".join(map(str, sheet.row_values(row_num))))
    return "\n".join(data)

def read_xlsx(file_path):
    workbook = load_workbook(file_path)
    sheet = workbook.active
    data = []
    for row in sheet.iter_rows(values_only=True):
        data.append("\t".join(map(str, row)))
    return "\n".join(data)

def load_files(directory):
    documents = []
    unread_documents = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)
    for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)
        try:
            if filename.endswith('.txt'):
                content = read_txt(file_path)
            elif filename.endswith('.pdf'):
                content = read_pdf(file_path)
            elif filename.endswith('.docx'):
                content = read_docx(file_path)
            elif filename.endswith('.xls'):
                content = read_xls(file_path)
            elif filename.endswith('.xlsx'):
                content = read_xlsx(file_path)
            else:
                print(f"Unsupported file format: {filename}")
                unread_documents.append(filename)
                continue
            doc = Document(page_content=content, metadata={"filename": filename})
            split_docs = text_splitter.split_documents([doc])
            documents.extend(split_docs)
        except Exception as e:
            print(f"Failed to process {filename}: {e}")
            unread_documents.append(filename)
    return documents, unread_documents

def initialize_models():
    # Initialize VertexAI models
    llm = VertexAI(model="gemini-pro", top_k=5, top_p=0.9, temperature=0.7, max_output_tokens=2048)
    embeddings = VertexAIEmbeddings(model_name="textembedding-gecko@003")
    return llm, embeddings

def create_and_save_faiss_index(documents, embeddings_model, batch_size=1000, index_path='faiss_index.bin', docs_path='documents.pkl'):
    all_embeddings = []
    all_documents = []

    index_to_docstore_id = {}

    for i in range(0, len(documents), batch_size):
        batch_docs = documents[i:i + batch_size]
        faiss_index = FAISS.from_documents(batch_docs, embeddings_model)
        all_embeddings.append(faiss_index.index.reconstruct_n(0, faiss_index.index.ntotal))
        all_documents.extend(batch_docs)
        for j, doc in enumerate(batch_docs):
            index_to_docstore_id[len(index_to_docstore_id)] = doc

    # Concatenate all embeddings
    embeddings = np.vstack(all_embeddings)
    
    # Create final FAISS index
    dimension = embeddings.shape[1]
    final_index = faiss.IndexFlatL2(dimension)
    final_index.add(embeddings)

    # Save final FAISS index
    faiss.write_index(final_index, index_path)
    
    # Save Document objects
    with open(docs_path, 'wb') as f:
        pickle.dump(all_documents, f)

    # Create Docstore
    docstore = Docstore(dict(enumerate(all_documents)))

    # Convert to langchain_community.vectorstores.faiss.FAISS object
    faiss_vectorstore = FAISS(index=final_index, docstore=docstore, embedding_function=embeddings_model, index_to_docstore_id=index_to_docstore_id)
    return faiss_vectorstore

def create_retriever(vectorstore):
    # Create retriever
    retriever = vectorstore.as_retriever()
    return retriever

def define_prompts():
    # Define prompts
    contextualize_q_system_prompt = "Given a chat history and the latest user question which might reference content in the chat history, formulate a standalone question which can be understood without the chat history. Do Not answer the question, just reformulate it if needed and otherwise return it as is."
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}"),
        ]
    )
    contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()

    qa_system_prompt = "You are an assistant for question answering tasks. Use only the following pieces of retrieved context to answer the question. If the question is not related to the context then don't answer, just say that you are not sure about that. If you don't know the answer, just say that you are not sure about that in 1 or 2 lines and strictly don't exceed more than that. Question: {question} Context: {context} Answer:"
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}"),
        ]
    )
    return contextualize_q_chain, qa_prompt

def define_chain(contextualize_q_chain, qa_prompt, retriever):
    # Define chain
    rag_chain = (
        RunnablePassthrough.assign(context=contextualize_q_chain | retriever | format_docs) | qa_prompt | llm
    )
    return rag_chain

def contextualized_question(input):
    if input.get("chat_history"):
        return contextualize_q_chain
    else:
        return input['question']

def format_docs(docs):
    formatted_docs = "\n\n".join(f"{doc.page_content} (Source: {doc.metadata['source']})" if 'source' in doc.metadata else f"{doc.page_content}" for doc in docs)
    return formatted_docs

# Main execution
if __name__ == "__main__":
    # Initialize models
    llm, embeddings = initialize_models()

    # Load files and create FAISS index
    documents, unread_documents = load_files("/path/to/your/directory")
    faiss_vectorstore = create_and_save_faiss_index(documents, embeddings)

    # Define prompts and chain
    contextualize_q_chain, qa_prompt = define_prompts()
    retriever = create_retriever(faiss_vectorstore)
    rag_chain = define_chain(contextualize_q_chain, qa_prompt, retriever)

    # Example query
    query = "Your query here"
    print(rag_chain.invoke({"chat_history": [], "question": query}))
